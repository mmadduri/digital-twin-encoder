{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3028b98",
   "metadata": {},
   "source": [
    "### Get Data Code\n",
    "\n",
    "### Last Updated: 12 Oct 2023\n",
    "\n",
    "### Authors: Momona Yamagami, Maneeshika Madduri\n",
    "\n",
    "### Goal: \n",
    "This code take the h5 files that the partcipant data was saved in and coverts it to a pickle file along with the experimental conditions. For the sake of memory, the h5 files were split into block 1 and block 2, each participant has block 1 and block 2 data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab4591ab-715c-4c9d-974f-4adec32a03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aopy \n",
    "#aopy as in Orsborn lab's data processing pipeline, \n",
    "#can be downloaded here https://github.com/aolabNeuro/analyze\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn \n",
    "import scipy\n",
    "from scipy.optimize import minimize,least_squares\n",
    "import copy as copy\n",
    "import time as tpy\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "seaborn.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ddb753",
   "metadata": {},
   "source": [
    "# Co-adaptive Myoelectric Interface Continuous Task data extraction\n",
    "## extract data and save to pandas dataframe\n",
    "\n",
    "- 14 participants (METAS106 - METAS119)\n",
    "- 2 blocks (BLOCK1, BLOCK2)\n",
    "- \n",
    "- `weiner_task_data` $\\rightarrow$ data that we want\n",
    "- `quattro_data` $\\rightarrow$ pretty sure this is raw EMG data\n",
    "\n",
    "ISSUE: can't analyze block 1 AND block 2 -- memory issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c0448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['METACPHS_S106', \n",
    "        'METACPHS_S107',\n",
    "        'METACPHS_S108', \n",
    "        'METACPHS_S109', \n",
    "        'METACPHS_S110', \n",
    "        'METACPHS_S111', \n",
    "        'METACPHS_S112',\n",
    "        'METACPHS_S113',\n",
    "        'METACPHS_S114',\n",
    "        'METACPHS_S115',\n",
    "        'METACPHS_S116',\n",
    "        'METACPHS_S117',\n",
    "        'METACPHS_S118',\n",
    "        'METACPHS_S119']\n",
    "\n",
    "# create dataframe\n",
    "var_names = ['refs','poss','dec_vels','int_vels','emgs','Ws','Hs','alphas', 'pDs','times','conditions']\n",
    "\n",
    "## BLOCK 1\n",
    "refs_block1 = {key: [] for key in keys} # re\n",
    "poss_block1 = {key: [] for key in keys}\n",
    "dec_vels_block1 = {key: [] for key in keys}                                            \n",
    "emgs_block1 = {key: [] for key in keys}\n",
    "int_vel_block1 = {key: [] for key in keys}\n",
    "Ws_block1 = {key: [] for key in keys}\n",
    "Hs_block1 = {key: [] for key in keys}\n",
    "alphas_block1 = {key: [] for key in keys}\n",
    "pDs_block1 = {key: [] for key in keys}\n",
    "times_block1 = {key: [] for key in keys}\n",
    "conditions_block1 = {key: [] for key in keys}\n",
    "\n",
    "## BLOCK 2\n",
    "refs_block2 = {key: [] for key in keys} # re\n",
    "poss_block2 = {key: [] for key in keys}\n",
    "dec_vels_block2 = {key: [] for key in keys}\n",
    "emgs_block2 = {key: [] for key in keys}\n",
    "int_vel_block2 = {key: [] for key in keys}\n",
    "Ws_block2 = {key: [] for key in keys}\n",
    "Hs_block2 = {key: [] for key in keys}\n",
    "alphas_block2 = {key: [] for key in keys}\n",
    "pDs_block2 = {key: [] for key in keys}\n",
    "times_block2 = {key: [] for key in keys}\n",
    "conditions_block2 = {key: [] for key in keys}\n",
    "# decoders_block1 = {key: [] for key in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24affba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['METACPHS_S106',\n",
       " 'METACPHS_S107',\n",
       " 'METACPHS_S108',\n",
       " 'METACPHS_S109',\n",
       " 'METACPHS_S110',\n",
       " 'METACPHS_S111',\n",
       " 'METACPHS_S112',\n",
       " 'METACPHS_S113',\n",
       " 'METACPHS_S114',\n",
       " 'METACPHS_S115',\n",
       " 'METACPHS_S116',\n",
       " 'METACPHS_S117',\n",
       " 'METACPHS_S118',\n",
       " 'METACPHS_S119']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the subjects? confirm the subject numbers\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49452e0",
   "metadata": {},
   "source": [
    "First need to sort files in order by learning rate and then decoder condtion. We expect this to be the order of files as they are saved:\n",
    "\n",
    "a_25_D_1 \n",
    "\n",
    "a_25_D_2\n",
    "\n",
    "a_25_D_5\n",
    "\n",
    "a_25_D_6\n",
    "\n",
    "a_75_D_3\n",
    "\n",
    "a_75_D_4\n",
    "\n",
    "a_75_D_7\n",
    "\n",
    "a_75_D_8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f475ab4",
   "metadata": {},
   "source": [
    "Then find the indices of the filenames required to sort the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73168950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory where the data is stored\n",
    "PATH =  '/Volumes/My Passport/cphs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65e453dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/My Passport/cphs/\n",
      "/Volumes/My Passport/cphs/METACPHS_S112/BLOCK2\n",
      "['weiner_task_data_METACPHS_S112_L2_a_25_D_1_BLOCK_2_pE_1e-06_pD_1e-03_pF_1e-07.h5', 'weiner_task_data_METACPHS_S112_L2_a_25_D_2_BLOCK_2_pE_1e-06_pD_1e-04_pF_1e-07.h5', 'weiner_task_data_METACPHS_S112_L2_a_25_D_5_BLOCK_2_pE_1e-06_pD_1e-03_pF_1e-07.h5', 'weiner_task_data_METACPHS_S112_L2_a_75_D_3_BLOCK_2_pE_1e-06_pD_1e-03_pF_1e-07.h5', 'weiner_task_data_METACPHS_S112_L2_a_75_D_4_BLOCK_2_pE_1e-06_pD_1e-04_pF_1e-07.h5', 'weiner_task_data_METACPHS_S112_L2_a_75_D_7_BLOCK_2_pE_1e-06_pD_1e-03_pF_1e-07.h5', 'weiner_task_data_METACPHS_S112_L2_a_75_D_8_BLOCK_2_pE_1e-06_pD_1e-04_pF_1e-07.h5', 'weiner_task_data_METACPHS_S112_L2_a_25_D_6_BLOCK_2_pE_1e-06_pD_1e-04_pF_1e-07.h5']\n",
      "sorted:\n",
      "['weiner_task_data_METACPHS_S112_L2_a_25_D_1_BLOCK_2_pE_1e-06_pD_1e-03_pF_1e-07.h5', 'weiner_task_data_METACPHS_S112_L2_a_25_D_2_BLOCK_2_pE_1e-06_pD_1e-04_pF_1e-07.h5', 'weiner_task_data_METACPHS_S112_L2_a_25_D_5_BLOCK_2_pE_1e-06_pD_1e-03_pF_1e-07.h5', 'weiner_task_data_METACPHS_S112_L2_a_25_D_6_BLOCK_2_pE_1e-06_pD_1e-04_pF_1e-07.h5', 'weiner_task_data_METACPHS_S112_L2_a_75_D_3_BLOCK_2_pE_1e-06_pD_1e-03_pF_1e-07.h5', 'weiner_task_data_METACPHS_S112_L2_a_75_D_4_BLOCK_2_pE_1e-06_pD_1e-04_pF_1e-07.h5', 'weiner_task_data_METACPHS_S112_L2_a_75_D_7_BLOCK_2_pE_1e-06_pD_1e-03_pF_1e-07.h5', 'weiner_task_data_METACPHS_S112_L2_a_75_D_8_BLOCK_2_pE_1e-06_pD_1e-04_pF_1e-07.h5']\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "print(PATH)\n",
    "data_dir = PATH + 'METACPHS_S112' +'/BLOCK2'\n",
    "print(data_dir)\n",
    "for name in glob.glob(data_dir + '/weiner_task_data*.h5'):\n",
    "    file_name = os.path.basename(name)\n",
    "    files.append(file_name)\n",
    "\n",
    "print(files)\n",
    "test_file = files[0]\n",
    "a_num = 7 # index of alpha\n",
    "d_num = 9 # index of decoder condition\n",
    "test_file.split(\"_\")[a_num] + test_file.split(\"_\")[d_num]  # choose indexes 7 and 9\n",
    "files = sorted(files, key = lambda x: x.split(\"_\")[a_num] + x.split(\"_\")[d_num])\n",
    "print(\"sorted:\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1599298f",
   "metadata": {},
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa58e3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze block 1\n",
      "analyzing METACPHS_S106\n",
      "analyzing METACPHS_S107\n",
      "analyzing METACPHS_S108\n",
      "analyzing METACPHS_S109\n",
      "analyzing METACPHS_S110\n",
      "analyzing METACPHS_S111\n",
      "analyzing METACPHS_S112\n",
      "analyzing METACPHS_S113\n",
      "analyzing METACPHS_S114\n",
      "analyzing METACPHS_S115\n",
      "analyzing METACPHS_S116\n",
      "analyzing METACPHS_S117\n",
      "analyzing METACPHS_S118\n",
      "analyzing METACPHS_S119\n"
     ]
    }
   ],
   "source": [
    "print('analyze block 1')\n",
    "for key in keys: # each subject\n",
    "    print('analyzing', key)\n",
    "    data_dir = PATH + key +'/BLOCK1' # get you to the folder\n",
    "    files = [] # set up list of files to sort\n",
    "    for name in glob.glob(data_dir + '/weiner_task_data*.h5'):\n",
    "        file_name = os.path.basename(name)\n",
    "        files.append(file_name)\n",
    "    \n",
    "\n",
    "    # sort the files in the order we expect\n",
    "    files = sorted(files, key = lambda x: x.split(\"_\")[a_num] + x.split(\"_\")[d_num])  \n",
    "\n",
    "    for file_name in files:\n",
    "        weiner_task_data, weiner_config = aopy.data.load_bmi3d_hdf_table(data_dir, file_name, 'weiner')\n",
    "        f = weiner_task_data\n",
    "        c = weiner_config\n",
    "#         print(f.dtype.names) # get the names of the fields\n",
    "        refs_block1[key].append(np.asarray(f['reference']))\n",
    "        poss_block1[key].append(np.asarray(f['decoded_position']))\n",
    "        dec_vels_block1[key].append(np.asarray(f['decoded_velocity']))\n",
    "        int_vel_block1[key].append(np.asarray(f['intended_velocity'])) # didn't seem to be saving\n",
    "        emgs_block1[key].append(np.asarray(f['filtered_emg']))\n",
    "        Ws_block1[key].append(np.asarray(f['weiner_filter_w']))\n",
    "        Hs_block1[key].append(np.asarray(f['weiner_filter_h'][0]))\n",
    "        alphas_block1[key].append(c['alpha'])\n",
    "        pDs_block1[key].append(c['lambdaD'])\n",
    "        time = np.asarray(f['timestamp']).flatten()\n",
    "        times_block1[key].append(time - time[0])\n",
    "        conditions_block1[key].append(file_name[-41:-38])\n",
    "    refs_block1[key] = np.asarray(refs_block1[key])\n",
    "    poss_block1[key] = np.asarray(poss_block1[key])\n",
    "    dec_vels_block1[key] = np.asarray(dec_vels_block1[key])\n",
    "    emgs_block1[key] = np.asarray(emgs_block1[key])\n",
    "    Ws_block1[key] = np.asarray(Ws_block1[key])\n",
    "    Hs_block1[key] = np.asarray(Hs_block1[key])\n",
    "    alphas_block1[key] = np.asarray(alphas_block1[key])\n",
    "    pDs_block1[key] = np.asarray(pDs_block1[key])\n",
    "    times_block1[key] = np.asarray(times_block1[key])\n",
    "    conditions_block1[key] = np.asarray(conditions_block1[key])\n",
    "    int_vel_block1[key] = np.asarray(int_vel_block1[key])\n",
    "\n",
    "    assert(np.all(conditions_block1[key] == ['D_1', 'D_2', 'D_5', 'D_6', 'D_3', 'D_4', 'D_7', 'D_8']) == True)\n",
    "    assert(len(conditions_block1[key])== 8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e790ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze block 2\n",
      "analyzing METACPHS_S106\n",
      "analyzing METACPHS_S107\n",
      "analyzing METACPHS_S108\n",
      "analyzing METACPHS_S109\n",
      "analyzing METACPHS_S110\n",
      "analyzing METACPHS_S111\n",
      "analyzing METACPHS_S112\n",
      "analyzing METACPHS_S113\n",
      "analyzing METACPHS_S114\n",
      "analyzing METACPHS_S115\n",
      "analyzing METACPHS_S116\n",
      "analyzing METACPHS_S117\n",
      "analyzing METACPHS_S118\n",
      "analyzing METACPHS_S119\n"
     ]
    }
   ],
   "source": [
    "# make sure files are in the order we want them to be in \n",
    "\n",
    "print('analyze block 2')\n",
    "for key in keys: # each subject\n",
    "    print('analyzing', key)\n",
    "    data_dir = PATH + key +'/BLOCK2' # get you to the folder\n",
    "    files = [] # set up list of files\n",
    "    for name in glob.glob(data_dir + '/weiner_task_data*.h5'):\n",
    "        file_name = os.path.basename(name)\n",
    "        files.append(file_name)\n",
    "\n",
    "    # sort files in order we expect\n",
    "    files = sorted(files, key = lambda x: x.split(\"_\")[a_num] + x.split(\"_\")[d_num])  \n",
    "\n",
    "    for file_name in files:\n",
    "        weiner_task_data, weiner_config = aopy.data.load_bmi3d_hdf_table(data_dir, file_name, 'weiner')\n",
    "        f = weiner_task_data\n",
    "        c = weiner_config\n",
    "        refs_block2[key].append(np.asarray(f['reference']))\n",
    "        poss_block2[key].append(np.asarray(f['decoded_position']))\n",
    "        dec_vels_block2[key].append(np.asarray(f['decoded_velocity']))\n",
    "        int_vel_block2[key].append(np.asarray(f['intended_velocity'])) # didn't seem to be saving\n",
    "        emgs_block2[key].append(np.asarray(f['filtered_emg']))\n",
    "        Ws_block2[key].append(np.asarray(f['weiner_filter_w']))\n",
    "        Hs_block2[key].append(np.asarray(f['weiner_filter_h'][0]))\n",
    "        alphas_block2[key].append(c['alpha'])\n",
    "        pDs_block2[key].append(c['lambdaD'])\n",
    "        time = np.asarray(f['timestamp']).flatten()\n",
    "        times_block2[key].append(time - time[0])\n",
    "        conditions_block2[key].append(file_name[-41:-38])\n",
    "    refs_block2[key] = np.asarray(refs_block2[key])\n",
    "    poss_block2[key] = np.asarray(poss_block2[key])\n",
    "    dec_vels_block2[key] = np.asarray(dec_vels_block2[key])\n",
    "    emgs_block2[key] = np.asarray(emgs_block2[key])\n",
    "    Ws_block2[key] = np.asarray(Ws_block2[key])\n",
    "    Hs_block2[key] = np.asarray(Hs_block2[key])\n",
    "    alphas_block2[key] = np.asarray(alphas_block2[key])\n",
    "    pDs_block2[key] = np.asarray(pDs_block2[key])\n",
    "    times_block2[key] = np.asarray(times_block2[key])\n",
    "    conditions_block2[key] = np.asarray(conditions_block2[key])\n",
    "    int_vel_block2[key] = np.asarray(int_vel_block2[key])\n",
    "    assert(np.all(conditions_block2[key] == ['D_1', 'D_2', 'D_5', 'D_6', 'D_3', 'D_4', 'D_7', 'D_8']) == True)\n",
    "    assert(len(conditions_block2[key])== 8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0ecbe32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/My Passport/cphs/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm the directory where we want to save the data\n",
    "PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da7a13db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [refs_block1, poss_block1, dec_vels_block1, int_vel_block1, emgs_block1, Ws_block1,Hs_block1,alphas_block1,pDs_block1,times_block1,conditions_block1]\n",
    "\n",
    "with open(PATH + 'continuous_full_data_block1_sorted.pickle','wb') as handle:\n",
    "    pickle.dump(data,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c8e0339",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [refs_block2, poss_block2, dec_vels_block2, int_vel_block2, emgs_block2, Ws_block2, Hs_block2, alphas_block2, pDs_block2, times_block2, conditions_block2]\n",
    "\n",
    "with open(PATH + 'continuous_full_data_block2_sorted.pickle','wb') as handle:\n",
    "    pickle.dump(data,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
